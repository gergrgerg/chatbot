{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gergrgerg/chatbot/blob/main/Copy_of_train_textbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKWFl9YI6RU8",
        "outputId": "bf512002-f147-414f-a99b-e9f8341a3d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyMuPDF\n",
            "Successfully installed pyMuPDF-1.25.5\n"
          ]
        }
      ],
      "source": [
        "#importing necessary libraries\n",
        "!pip install pyMuPDF\n",
        "import urllib.request\n",
        "import fitz\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import os\n",
        "from sklearn.neighbors import NearestNeighbors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Pat0rreATkXU"
      },
      "outputs": [],
      "source": [
        "# defining the required functions\n",
        "def download_pdf(url, output_path):\n",
        "    urllib.request.urlretrieve(url, output_path)\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def pdf_to_text(path, start_page=1, end_page=None):\n",
        "    doc = fitz.open(path)\n",
        "    total_pages = doc.page_count\n",
        "\n",
        "    if end_page is None:\n",
        "        end_page = total_pages\n",
        "\n",
        "    text_list = []\n",
        "\n",
        "    for i in range(start_page-1, end_page):\n",
        "        text = doc.load_page(i).get_text(\"text\")\n",
        "        text = preprocess(text)\n",
        "        text_list.append(text)\n",
        "\n",
        "    doc.close()\n",
        "    return text_list\n",
        "\n",
        "\n",
        "def text_to_chunks(texts, word_length=150, start_page=1):\n",
        "    text_toks = [t.split(' ') for t in texts]\n",
        "    page_nums = []\n",
        "    chunks = []\n",
        "\n",
        "    for idx, words in enumerate(text_toks):\n",
        "        for i in range(0, len(words), word_length):\n",
        "            chunk = words[i:i+word_length]\n",
        "            if (i+word_length) > len(words) and (len(chunk) < word_length) and (\n",
        "                len(text_toks) != (idx+1)):\n",
        "                text_toks[idx+1] = chunk + text_toks[idx+1]\n",
        "                continue\n",
        "            chunk = ' '.join(chunk).strip()\n",
        "            chunk = f'[{idx+start_page}]' + ' ' + '\"' + chunk + '\"'\n",
        "            chunks.append(chunk)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x_ubpjtSUONy"
      },
      "outputs": [],
      "source": [
        "# loading the textbook\n",
        "download_pdf('https://www.gopalancolleges.com/gpuc/pdf/pu-2-science/II%20PUC%20Eng%20TB.pdf','book.pdf')\n",
        "#converting into .txt format\n",
        "pages=pdf_to_text('book.pdf')\n",
        "#creating chunks from the text generated\n",
        "chunks=text_to_chunks(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKlCpf4QXYlL",
        "outputId": "38226026-f8dc-4584-8411-0f50d72c237e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the toalwords are  72430\n",
            "length of chunks is 483\n"
          ]
        }
      ],
      "source": [
        "total_words=0\n",
        "for page in pages:\n",
        "  total_words +=len(page.split(' '))\n",
        "\n",
        "print(\"the toalwords are \", total_words)\n",
        "print(\"length of chunks is\",len(chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb-SWy6XX0P_",
        "outputId": "e7a0da6a-8734-4b03-edb7-d7c64d5704b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[127] \"2016                                                                                                         2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16\"\n",
            "\n",
            "[127] \"2016                                                                                                         2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16\"\n",
            "\n",
            "[127] \"2016                                                                                                         2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16\"\n",
            "\n",
            "[127] \"2016                                                                                                         2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16\"\n",
            "\n",
            "[176] \"2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16  2015-16\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#displaying random number of chunks\n",
        "import random\n",
        "random.seed(123)\n",
        "random_idxs = random.choices(range(0, len(chunks)), k=5)\n",
        "\n",
        "for idx in random_idxs:\n",
        "  print(chunks[idx])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_XZN9AFVaShQ"
      },
      "outputs": [],
      "source": [
        "#defining the class for creation of vector space\n",
        "class SemanticSearch:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.use = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
        "        self.fitted = False\n",
        "\n",
        "\n",
        "    def fit(self, data, batch=1000, n_neighbors=5):\n",
        "        self.data = data\n",
        "        self.embeddings = self.get_text_embedding(data, batch=batch)\n",
        "        n_neighbors = min(n_neighbors, len(self.embeddings))\n",
        "        self.nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "        self.nn.fit(self.embeddings)\n",
        "        self.fitted = True\n",
        "\n",
        "\n",
        "    def __call__(self, text, return_data=True):\n",
        "        inp_emb = self.use([text])\n",
        "        neighbors = self.nn.kneighbors(inp_emb, return_distance=False)[0]\n",
        "\n",
        "        if return_data:\n",
        "            return [self.data[i] for i in neighbors]\n",
        "        else:\n",
        "            return neighbors\n",
        "\n",
        "\n",
        "    def get_text_embedding(self, texts, batch=1000):\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch):\n",
        "            text_batch = texts[i:(i+batch)]\n",
        "            emb_batch = self.use(text_batch)\n",
        "            embeddings.append(emb_batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnWLACBZcv6p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d74e653-748a-4279-f967-32110b850de3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_ap3Xx8aVDB"
      },
      "outputs": [],
      "source": [
        "recommender = SemanticSearch()\n",
        "recommender.fit(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzmOAqykKYBR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "gKlLkaOTawi0",
        "outputId": "397759d5-32f4-4baa-a2f0-56b71ebcd9cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'recommender' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4eb0efe2d24b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecommender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'what is romeo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'recommender' is not defined"
          ]
        }
      ],
      "source": [
        "recommender('what is ')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}